---
layout: post
title:  "Differential geometry-based techniques for characterization
of boundary roughness of pulmonary nodules in CT images"
place: INTERSPEECH 2019
authors: Ashis Kumar Dhara, Sudipta Mukhopadhyay, Pramit Saha, Mandeep Garg, Niranjan Khandelwal
date:   2019-11-02 10:00:00 +0200
image: /images/mixednet.jpg
categories: deep-learning
paper: https://arxiv.org/pdf/1904.05746.pdf
arxiv:
code: 
poster: 
comments:
year: 2019
---

<style>
@media (max-width: 1000px) {
    .container {
        flex-direction: column;
        align-items: left;
    }
</style>


<div class="container" style="display: flex; align-items: center;">
    <div class="image" style="flex: 1; margin-right: 1cm;">
        <img src="/images/mixednet.jpg" alt="Image" style="max-width:100%; height:auto;">
    </div>
</div>

**Abstract**

Speech-related Brain Computer Interface (BCI) technologies provide effective vocal communication strategies for controlling devices through speech commands interpreted from brain
signals. In order to infer imagined speech from active thoughts,
we propose a novel hierarchical deep learning BCI system for
subject-independent classification of 11 speech tokens including phonemes and words. Our novel approach exploits predicted articulatory information of six phonological categories
(e.g., nasal, bilabial) as an intermediate step for classifying the
phonemes and words, thereby finding discriminative signal responsible for natural speech synthesis. The proposed network is
composed of hierarchical combination of spatial and temporal
CNN cascaded with a deep autoencoder. Our best models on the
KARA database achieve an average accuracy of 83.42% across
the six different binary phonological classification tasks, and
53.36% for the individual token identification task, significantly
outperforming our baselines. Ultimately, our work suggests the
possible existence of a brain imagery footprint for the underlying articulatory movement related to different sounds that can
be used to aid imagined speech decoding.

Dive into our research!

<a href="https://arxiv.org/pdf/1904.05746">&#x1F4C4; Paper</a> 
